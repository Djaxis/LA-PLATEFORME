{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Djaxis/LA-PLATEFORME/blob/main/scraplaplate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STxo0RER8Ai-"
      },
      "source": [
        "### Résumé des tâches avec les technologies NLP utilisées et leur utilité\n",
        "\n",
        "---\n",
        "\n",
        "**Assurez-vous d'avoir téléchargé les ressources nécessaires.**\n",
        "\n",
        "- **Utilité :** Télécharger les ressources nécessaires pour le traitement de texte avec NLTK, notamment les ponctuations et les mots vides (stopwords).\n",
        "\n",
        "---\n",
        "\n",
        "**Fonction : `get_all_links`**\n",
        "\n",
        "- **Utilité :** Collecter tous les liens internes pour le crawling des pages.\n",
        "\n",
        "- **Tâches :**\n",
        "  - Récupérer tous les liens internes sur la page donnée.\n",
        "  - Trouver toutes les balises `<a>` avec un attribut `href`.\n",
        "  - Vérifier si le lien est relatif et le convertir en URL absolue.\n",
        "  - Vérifier si le lien commence par l'URL de base.\n",
        "\n",
        "- **Technologies NLP :** Non applicable (NA).\n",
        "\n",
        "---\n",
        "\n",
        "**Fonction : `clean_soup`**\n",
        "\n",
        "- **Utilité :** Nettoyer le contenu HTML pour ne conserver que les informations pertinentes.\n",
        "\n",
        "- **Tâches :**\n",
        "  - Nettoyer la soup en enlevant les éléments indésirables.\n",
        "  - Supprimer les balises `<script>` et `<style>`.\n",
        "  - Supprimer les commentaires.\n",
        "  - Supprimer les balises de publicité (exemple: `<div class=\"ad\">`).\n",
        "  - Supprimer les balises de navigation (exemple: `<nav>`).\n",
        "  - Supprimer les balises de pied de page (exemple: `<footer>`).\n",
        "  - Supprimer les balises de header (exemple: `<header>`).\n",
        "\n",
        "- **Technologies NLP :** Non applicable (NA).\n",
        "\n",
        "---\n",
        "\n",
        "**Fonction : `generate_questions`**\n",
        "\n",
        "- **Utilité :** Transformer des phrases en questions pour générer des paires question-réponse.\n",
        "\n",
        "- **Tâches :**\n",
        "  - Générer des questions à partir des phrases données.\n",
        "  - Tokenizer la phrase en mots.\n",
        "  - Filtrer les mots vides (stop words).\n",
        "  - Reconstituer la phrase sans les mots vides.\n",
        "  - Ajouter un point d'interrogation à la fin pour former une question.\n",
        "\n",
        "- **Technologies NLP :**\n",
        "  - `nltk.tokenize` (pour tokenizer les phrases en mots).\n",
        "  - `nltk.corpus.stopwords` (pour filtrer les mots vides).\n",
        "  - `nltk.tokenize.treebank.TreebankWordDetokenizer` (pour reconstituer les phrases).\n",
        "\n",
        "---\n",
        "\n",
        "**Fonction : `scrape_page`**\n",
        "\n",
        "- **Utilité :** Extraire le contenu textuel des pages web et générer des paires question-réponse.\n",
        "\n",
        "- **Tâches :**\n",
        "  - Scraper les phrases de la page donnée.\n",
        "  - Envoyer une requête GET à l'URL.\n",
        "  - Vérifier si la requête a réussi (code 200).\n",
        "  - Analyser le contenu HTML avec Beautiful Soup.\n",
        "  - Nettoyer la soup.\n",
        "  - Trouver tous les paragraphes (balises `<p>`).\n",
        "  - Extraire le texte de chaque paragraphe et les retourner.\n",
        "  - Générer des questions à partir des phrases.\n",
        "  - Associer chaque question à sa phrase originale comme réponse.\n",
        "\n",
        "- **Technologies NLP :**\n",
        "  - Beautiful Soup (pour analyser le contenu HTML).\n",
        "  - `generate_questions` (utilise les technologies NLP mentionnées ci-dessus).\n",
        "\n",
        "---\n",
        "\n",
        "**Fonction : `crawl_site`**\n",
        "\n",
        "- **Utilité :** Automatiser le processus de scraping et de génération de questions-réponses pour toutes les pages d'un site web.\n",
        "\n",
        "- **Tâches :**\n",
        "  - Crawl toutes les pages du site à partir de l'URL de départ.\n",
        "  - Ensemble des URLs déjà visitées.\n",
        "  - Liste des URLs à visiter.\n",
        "  - Dictionnaire pour stocker les questions-réponses par page.\n",
        "  - Prendre la première URL de la liste.\n",
        "  - Passer à l'URL suivante si elle a déjà été visitée.\n",
        "  - Ajouter l'URL à l'ensemble des URLs visitées.\n",
        "  - Scraper la page actuelle.\n",
        "  - Ajouter les questions-réponses générées au dictionnaire.\n",
        "  - Récupérer tous les liens internes de la page actuelle.\n",
        "  - Ajouter les nouveaux liens à la liste des URLs à visiter.\n",
        "\n",
        "- **Technologies NLP :**\n",
        "  - Utilise `scrape_page` et ses technologies associées.\n",
        "\n",
        "---\n",
        "\n",
        "**Section principale du code**\n",
        "\n",
        "- **Utilité :** Initialiser le processus de scraping et stocker les résultats.\n",
        "\n",
        "- **Tâches :**\n",
        "  - URL de la première page à scraper.\n",
        "  - Scraper toutes les pages accessibles via les menus.\n",
        "  - Enregistrer toutes les questions et réponses dans un fichier texte, classées par pages.\n",
        "\n",
        "- **Technologies NLP :** Utilise `crawl_site` et ses technologies associées.\n",
        "\n",
        "---\n",
        "\n",
        "**Exécution finale**\n",
        "\n",
        "- **Utilité :** Finaliser et organiser les résultats du scraping.\n",
        "\n",
        "- **Tâches :**\n",
        "  - Ajouter une ligne vide entre les pages.\n",
        "  - Indiquer que le scraping est terminé.\n",
        "\n",
        "- **Technologies NLP :** Non applicable (NA)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AiunRWde8AjO"
      },
      "outputs": [],
      "source": [
        "! pip install requests beautifulsoup4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5iz0Pfcp8AjX",
        "outputId": "b689e2eb-845d-4e08-d777-ca972c551683"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\change\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\change\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scraping https://laplateforme.io/qui-sommes-nous/\n",
            "Scraping https://laplateforme.io\n",
            "Scraping https://laplateforme.io/journees-portes-ouvertes/\n",
            "Scraping https://laplateforme.io/candidatures-cursus/\n",
            "Scraping https://laplateforme.io/telechargement-brochure/\n",
            "Scraping https://laplateforme.io/abonnement-newsletter/\n",
            "Scraping https://laplateforme.io/bachelor-it/\n",
            "Scraping https://laplateforme.io/master-of-science/\n",
            "Scraping https://laplateforme.io/ai-school/centrale-digital-lab/\n",
            "Scraping https://laplateforme.io/innovation-lab/\n",
            "Scraping https://laplateforme.io/newsletter-ia-n1/\n",
            "Scraping https://laplateforme.io/les-etudiants-de-la-plateforme_-remportent-la-regate-des-minots-2024/\n",
            "Scraping https://laplateforme.io/business-solution-special-cybersecurite-hacking/\n",
            "Scraping https://laplateforme.io/business-solution-special-martigues/\n",
            "Scraping https://laplateforme.io/retrospective-business-solution-special-ia-data/\n",
            "Scraping https://laplateforme.io/category/actualites/\n",
            "Scraping https://laplateforme.io/category/communiques-de-presse/\n",
            "Scraping https://laplateforme.io/category/evenements/\n",
            "Scraping https://laplateforme.io/category/interview/\n",
            "Scraping https://laplateforme.io/category/non-classe/\n",
            "Scraping https://laplateforme.io/category/webinaires/\n",
            "Scraping https://laplateforme.io/bachelor-it\n",
            "Scraping https://laplateforme.io/bachelor-it/init-bachelor/\n",
            "Scraping https://laplateforme.io/bachelor-it/developpeur-en-intelligence-artificielle/\n",
            "Scraping https://laplateforme.io/bachelor-it/developpeur-securite/\n",
            "Scraping https://laplateforme.io/developpeur-systemes-immersifs-image-numerique/\n",
            "Scraping https://laplateforme.io/bachelor-it/administration-securite-des-systemes-dinformations/\n",
            "Scraping https://laplateforme.io/bachelor-it/concepteur-developpeur-web/\n",
            "Scraping https://laplateforme.io/msc-it-business-ingenierie-du-web/\n",
            "Scraping https://laplateforme.io/msc-it-business-ia-data/\n",
            "Scraping https://laplateforme.io/cyber-security-school/formation-data-protection-officer/\n",
            "Scraping https://laplateforme.io/informations/\n",
            "Scraping https://laplateforme.io/ai-school/\n",
            "Scraping https://laplateforme.io/innovation-lab/habiter-demain-usage-urbain/\n",
            "Scraping https://laplateforme.io/innovation-lab/reseaux-sociaux-web-decentralise/\n",
            "Scraping https://laplateforme.io/abonnement-newsletter-ia/\n",
            "Scraping https://laplateforme.io/cedric-messeguer-rejoint-les-equipes-du-lab-cyber-et-de-la-plateforme/\n",
            "Scraping https://laplateforme.io/les-etudiants-de-la-plateforme-au-coeur-du-projet-sistemic/\n",
            "Scraping https://laplateforme.io/les-talents-en-recherche-dalternance-edition-2-2024/\n",
            "Scraping https://laplateforme.io/wp-content/uploads/2024/05/Hugo-Moreskh.pdf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scraping https://laplateforme.io/wp-content/uploads/2024/05/Imrane-Bendassi.pdf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scraping https://laplateforme.io/les-talents-en-recherche-dalternance-edition-1-2024/#BachelorIT-Web\n",
            "Scraping https://laplateforme.io/bachelor-it/developpeur-logiciel/\n",
            "Scraping https://laplateforme.io/wp-content/uploads/2024/05/cv_walid_saadelkhalk.pdf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scraping https://laplateforme.io/wp-content/uploads/2024/05/CV_Thanh_Lemelle.pdf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scraping https://laplateforme.io/les-talents-en-recherche-dalternance-edition-1-2024/#BachelorIT-Logiciel\n",
            "Scraping https://laplateforme.io/wp-content/uploads/2024/05/CLAVIS_TOM_CV.pdf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scraping https://laplateforme.io/wp-content/uploads/2024/05/BOUMEDIENE-MANSOUR-9.pdf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scraping https://laplateforme.io/les-talents-en-recherche-dalternance-edition-1-2024/#BachelorIT-Cyber\n",
            "Scraping https://laplateforme.io/wp-content/uploads/2024/05/ayoub-abderrahmane.pdf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scraping https://laplateforme.io/wp-content/uploads/2024/05/chabab-ilyes.pdf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scraping https://laplateforme.io/les-talents-en-recherche-dalternance-edition-1-2024/#BachelorIT-IA\n",
            "Scraping https://laplateforme.io/les-talents-en-recherche-dalternance-edition-1-2024/\n",
            "Scraping https://laplateforme.io/wp-content/uploads/2024/04/CV_Arthur-clerc.pdf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scraping https://laplateforme.io/wp-content/uploads/2024/04/CV-Johnathan-LE-PLAT.pdf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scraping https://laplateforme.io/wp-content/uploads/2024/04/CV-Lucy-MADEC.pdf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scraping https://laplateforme.io/wp-content/uploads/2024/04/CV-Vanny-LAMORTE.pdf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scraping https://laplateforme.io/wp-content/uploads/2024/04/CV-Mathis-SERRA.pdf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scraping https://laplateforme.io/wp-content/uploads/2024/04/CV-Anthony-Yrles.pdf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scraping https://laplateforme.io/wp-content/uploads/2024/04/CV-Maysa-Bik.pdf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scraping https://laplateforme.io/wp-content/uploads/2024/04/CV-RIJA-RASOANAIVO.pdf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scraping https://laplateforme.io/wp-content/uploads/2024/04/cv-MANON-RITTLING.pdf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scraping https://laplateforme.io/wp-content/uploads/2024/04/cv-Ines-Lorquet.pdf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scraping https://laplateforme.io/wp-content/uploads/2024/04/cv-Helio-Aubrun.pdf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scraping https://laplateforme.io/wp-content/uploads/2024/04/CV-Leo-Carrey.pdf\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup, Comment\n",
        "from urllib.parse import urljoin\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
        "\n",
        "# Assurez-vous d'avoir téléchargé les ressources nécessaires\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def get_all_links(soup, base_url):\n",
        "    \"\"\"Récupérer tous les liens internes sur la page donnée.\"\"\"\n",
        "    links = []\n",
        "    # Trouver toutes les balises <a> avec un attribut href\n",
        "    for link in soup.find_all('a', href=True):\n",
        "        href = link['href']\n",
        "        # Vérifier si le lien est relatif et le convertir en URL absolue\n",
        "        if href.startswith('/'):\n",
        "            full_url = urljoin(base_url, href)\n",
        "            links.append(full_url)\n",
        "        # Vérifier si le lien commence par l'URL de base\n",
        "        elif href.startswith(base_url):\n",
        "            links.append(href)\n",
        "    return links\n",
        "\n",
        "def clean_soup(soup):\n",
        "    \"\"\"Nettoyer la soup en enlevant les éléments indésirables.\"\"\"\n",
        "    # Supprimer les balises <script> et <style>\n",
        "    for script_or_style in soup(['script', 'style']):\n",
        "        script_or_style.decompose()\n",
        "\n",
        "    # Supprimer les commentaires\n",
        "    for comment in soup.find_all(string=lambda text: isinstance(text, Comment)):\n",
        "        comment.extract()\n",
        "\n",
        "    # Supprimer les balises de publicité (exemple: <div class=\"ad\">)\n",
        "    for ad in soup.find_all(class_='ad'):\n",
        "        ad.decompose()\n",
        "\n",
        "    # Supprimer les balises de navigation (exemple: <nav>)\n",
        "    for nav in soup.find_all('nav'):\n",
        "        nav.decompose()\n",
        "\n",
        "    # Supprimer les balises de pied de page (exemple: <footer>)\n",
        "    for footer in soup.find_all('footer'):\n",
        "        footer.decompose()\n",
        "\n",
        "    # Supprimer les balises de header (exemple: <header>)\n",
        "    for header in soup.find_all('header'):\n",
        "        header.decompose()\n",
        "\n",
        "    return soup\n",
        "\n",
        "def generate_questions(phrases):\n",
        "    \"\"\"Générer des questions à partir des phrases données.\"\"\"\n",
        "    questions = []\n",
        "    stop_words = set(stopwords.words('french'))\n",
        "\n",
        "    for phrase in phrases:\n",
        "        # Tokenizer la phrase en mots\n",
        "        words = nltk.word_tokenize(phrase, language='french')\n",
        "\n",
        "        # Filtrer les mots vides (stop words)\n",
        "        filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "\n",
        "        # Reconstituer la phrase sans les mots vides\n",
        "        filtered_phrase = TreebankWordDetokenizer().detokenize(filtered_words)\n",
        "\n",
        "        # Ajouter un point d'interrogation à la fin pour former une question\n",
        "        question = f\"{filtered_phrase} ?\"\n",
        "        questions.append(question)\n",
        "\n",
        "    return questions\n",
        "\n",
        "def scrape_page(url):\n",
        "    \"\"\"Scraper les phrases de la page donnée.\"\"\"\n",
        "    try:\n",
        "        # Envoyer une requête GET à l'URL\n",
        "        response = requests.get(url)\n",
        "        # Vérifier si la requête a réussi (code 200)\n",
        "        response.raise_for_status()\n",
        "    except requests.RequestException as e:\n",
        "        print(f'La requête a échoué pour {url} avec l\\'erreur : {e}')\n",
        "        return [], None\n",
        "\n",
        "    # Analyser le contenu HTML avec Beautiful Soup\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Nettoyer la soup\n",
        "    soup = clean_soup(soup)\n",
        "\n",
        "    # Trouver tous les paragraphes (balises <p>)\n",
        "    paragraphs = soup.find_all('p')\n",
        "\n",
        "    # Extraire le texte de chaque paragraphe et les retourner\n",
        "    phrases = [paragraph.get_text() for paragraph in paragraphs]\n",
        "\n",
        "    # Générer des questions à partir des phrases\n",
        "    questions = generate_questions(phrases)\n",
        "\n",
        "    # Associer chaque question à sa phrase originale comme réponse\n",
        "    qa_pairs = list(zip(questions, phrases))\n",
        "\n",
        "    return qa_pairs, soup\n",
        "\n",
        "def crawl_site(start_url, base_url):\n",
        "    \"\"\"Crawl toutes les pages du site à partir de l'URL de départ.\"\"\"\n",
        "    visited = set()  # Ensemble des URLs déjà visitées\n",
        "    to_visit = [start_url]  # Liste des URLs à visiter\n",
        "    all_qa_pairs_by_page = {}  # Dictionnaire pour stocker les questions-réponses par page\n",
        "\n",
        "    while to_visit:\n",
        "        current_url = to_visit.pop(0)  # Prendre la première URL de la liste\n",
        "        if current_url in visited:\n",
        "            continue  # Passer à l'URL suivante si elle a déjà été visitée\n",
        "        print(f'Scraping {current_url}')\n",
        "        visited.add(current_url)  # Ajouter l'URL à l'ensemble des URLs visitées\n",
        "\n",
        "        # Scraper la page actuelle\n",
        "        qa_pairs, soup = scrape_page(current_url)\n",
        "        all_qa_pairs_by_page[current_url] = qa_pairs  # Ajouter les questions-réponses générées au dictionnaire\n",
        "\n",
        "        if soup:\n",
        "            # Récupérer tous les liens internes de la page actuelle\n",
        "            links = get_all_links(soup, base_url)\n",
        "            for link in links:\n",
        "                if link not in visited and link not in to_visit:\n",
        "                    to_visit.append(link)  # Ajouter les nouveaux liens à la liste des URLs à visiter\n",
        "\n",
        "    return all_qa_pairs_by_page\n",
        "\n",
        "# URL de la première page à scraper\n",
        "base_url = 'https://laplateforme.io'\n",
        "start_url = base_url + '/qui-sommes-nous/'\n",
        "\n",
        "# Scraper toutes les pages accessibles via les menus\n",
        "all_qa_pairs_by_page = crawl_site(start_url, base_url)\n",
        "\n",
        "# Enregistrer toutes les questions et réponses dans un fichier texte, classées par pages\n",
        "with open('all_questions_answers.txt', 'w') as file:\n",
        "    for page_url, qa_pairs in all_qa_pairs_by_page.items():\n",
        "        file.write(f'Page URL: {page_url}\\n')\n",
        "        for question, answer in qa_pairs:\n",
        "            file.write(f'Q: {question}\\n')\n",
        "            file.write(f'A: {answer}\\n')\n",
        "        file.write('\\n')  # Ajouter une ligne vide entre les pages\n",
        "\n",
        "print('Scraping terminé.')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}